So:

* the CSV file must have the max row size, number of blocks in a page and gap blocks ratio specified. This comes at the end of the headers on the first row and is a cell starting with #PADDING(512,100,4)
* our parser understands the padding and can re-pad every time it goes wrong
* Writes are locked with a lock, either both reads and writes are locked during a repadding. Or a different file is written to. I think read and write locks should automatically always be obtained if there is a chance data could be written. Or any data about to be written should be confirmed with a read first.
* The block size, schema information and padding can be changed if the data is found to be wrong. They must still be human readable.



Also need the number of primary key columns and theit order.






* We have an index.pad file which is touched when the csv file is changed if we expect the padding to be right. Then if a padding error occurs, we know that we can't trust either the schema padding or both, so we re-pad the whole file. Otherwise, we just re-pad the bit that is wrong. Not sure about this - surely we re-pad every time
* 



















"Year","Month","Day","Name","Age"
2005,"05","03",james-rules,12
2005,"08","04",jamesg     ,12
2005,"08","04","j,a m"    ,12

Schema:

    schema = [
        ['Year', NUMBER, NO_LEADING_ZERO, 6],
        ['Month', NUMBER, LEADING_ZERO, 4],
        ['Day', NUMBER, LEADING_ZERO, 4],
        ['Name', UNICODE, None, 32],
        ['End', UNICODE, None, 1],
    ]
    data = [
        [2005, '05', '06', 'james'],
        [2005, '05', '06', 'james Gardner', '4'],
    ]

Do we care about whether it has a primary key? Do we put that in the schema.

Primary Key: Year DESC, Month DESC, Day DESC, Name ASC
Unique: Name

Year: [unicode ascii leading-zero-without-numbers-possible acsii-no-delimeter number] [max=4, average=100]

Always arrange the columns so that:
* the ones without fixed padding come first

Arrange the rows such that:
* we stick to the sort order where possible
* new rows are allowed at the end

Binary tree:


First column:
Could contain a status field specifying "inplace", "new", "deleted", "draft" etc



Questions:
* How to identify unsorted fields - do we need to?
* Lockfiles


data.txt
index.txt
schema.txt
lockfile.txt

Commands data2csv 
         data2xls
         data2json -> Empty fields map to null, empty fields with empty quoted strings map to null? Or just map all to an empty string. Or just don't have keys for empty strings?

reindex -> Re-writes all the padding, and the index file. Need to look at how mmap works.










So, rather than having an index, let's say that we don't care about reading 20 blocks.

If the blocksize is 4k, that means we always just 

Padding is always equal to the length of the record, rounded up to the next power of 2.

Let's say that the maxium length of an individual entry is 4k.

After 8 blocks, we leave an 16 block gap. Inserts can happen in this gap although hopefully they won't because we've use a time-based primary key.

This means to find pretty much the last record in the file, we take the size, and work back to the previous 8 block boundary. We can then read the primary key. If we don't find it, we re-write the entire CSV, assuming it to be corrupt.

We then guess based on a linear distribution, which block the next access might be at. We keep guessing until we find the 8 block chunk we want. Then we iterate through it, building an index in memory for future use if required.

If we fill the 8 block gap of one block we look at the 8 block gap of the next. If there is enough of a gap we re-write both blocks leaving equal space in each. If we have to re-write more than two blocks, we give up and just re-write the whole lot!

If we aren't sure, we read from slightly earlier, because the kernel read aheadssystem might read the next block ready for us anyway.

Also, let's agree a convention of CamelCase for headers with a .type for type.  eg Name.md [safeascii5] would be a Name max 5 ascii characters in md format.







If we always quote the first thing in any row we can always identify a clean row by looking at the previous CRLF, checking the subsequent character is a " and a character that isn't another " unless it is followed by ",. Otherwise, we know that what we've found is part of an existing row that goes over the block limit. In that case though how do we find the next clean block?

We just have to keep looking forward until we find it I suppose. Or we ban rows longer than the block size.



Triggering a repad when node reads fail. At the moment, the \r\n forms part of the last block. We want to make it part of the next block.

Could call infinite redirect to the browser whilst re-build takes place

The optimiser basically has no effect as recenty accessed rows are in the system memory anyway.
What about caching the last rows? Probably not worth it -> Don't overcomplicate.
